{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MNIST 데이터셋 \n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.mlp1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.mlp2 = nn.Linear(hidden_size, num_classes)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mlp1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.mlp2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(input_size=28*28*1, hidden_size=100, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adam Optimizer\n",
    "* lr : learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELoss = nn.CrossEntropyLoss()\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 최적화 알고리즘 class 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 순전파(forward propagation) :뉴럴 네트워크 모델의 입력층부터 출력층까지 순서대로 변수들을 계산하고 저장하는 것을 의미\n",
    "  * 딥러닝이 학습을 하면서 자신만의 답을 출력하는 과정이다. 모델에서 정답을 뽑아내는 과정\n",
    "* 역전파(back propagation) : 순전파 (Feedforward) 알고리즘 에서 발생한 오차를 줄이기 위해 새로운 가중치를 업데이트하고, 새로운 가중치로 다시 학습하는 과정\n",
    "  * 중간 변수와 파라미터에 대한 그래디언트(gradient)를 반대 방향으로 계산하고 저장\n",
    "  * loss가 작아지는 최적의 경로는 찾을 수 없지만, 지금 현재 시점에서 loss를 작게 만드는 최적의 방향 => 그레디언트의 반대 방향으로\n",
    "  * 그레디언트(gradient) : 벡터가 error space에서 가장 급격하게 에러가 증가하는 방향성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iteration : 469\n",
      "Epoch [1/3], Loss: 0.3107\n",
      "Epoch [2/3], Loss: 0.2814\n",
      "Epoch [3/3], Loss: 0.1423\n"
     ]
    }
   ],
   "source": [
    "# 뉴럴 네트워크 모델 학습\n",
    "total_epochs = 3\n",
    "print('number of iteration :', len(train_loader)) # 몇 개의 iter로 되어있는가\n",
    "# epoch : 모든 데이터를 한 번 학습하는 단위\n",
    "for epoch in range(total_epochs):\n",
    "    # iteration : 한 'mini-batch' 단위의 데이터를 학습하는 단위\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # images : [mini-batch, 1, 28, 28]\n",
    "        # labels : [mini-batch]\n",
    "        images = images.reshape(-1, 28*28).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass : input을 넣을 때 ouput을 내는 weight 구하는 과정\n",
    "        outputs = model(images)\n",
    "        ce_loss = CELoss(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        adam_optimizer.zero_grad() # 다양한 optimization 기법 적용 가능 => gradient 초기화\n",
    "        ce_loss.backward() # Back propagation\n",
    "        adam_optimizer.step() # optimizer 작동\n",
    "            \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, total_epochs, ce_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 성능 테스트 => back propgation 과정을 수행하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95.82 %\n"
     ]
    }
   ],
   "source": [
    "# 학습이 끝난 후 모델 성능 테스트\n",
    "# test에서는 back propagation 작업을 하지 않으므로 gradient를 계산하지 않도록 함 - 메모리의 효율성을 위해\n",
    "with torch.no_grad(): # gradient 계산하지 않도록 하는 코드\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1) # 모델이 뽑은 output 중에 최대값\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item() # 예측을 잘 한 것\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9582"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델을 model.ckpt라는 이름으로 저장\n",
    "torch.save(model.state_dict(), 'model_basic.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최적화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent, momentum, Adagrad, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Stochastic Gradient Descent with momentum\n",
    "sgd_with_momentum_optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adagrad\n",
    "Adagrad_optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "# RMSprop\n",
    "RMSprop_optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "# Adam\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.1241\n",
      "Epoch [2/3], Loss: 0.1034\n",
      "Epoch [3/3], Loss: 0.2531\n",
      "Epoch [4/3], Loss: 0.1083\n",
      "Epoch [5/3], Loss: 0.1863\n",
      "Epoch [6/3], Loss: 0.1656\n",
      "Epoch [7/3], Loss: 0.1415\n",
      "Epoch [8/3], Loss: 0.1886\n",
      "Epoch [9/3], Loss: 0.1162\n",
      "Epoch [10/3], Loss: 0.0695\n",
      "Epoch [11/3], Loss: 0.0575\n",
      "Epoch [12/3], Loss: 0.1460\n",
      "Epoch [13/3], Loss: 0.0901\n",
      "Epoch [14/3], Loss: 0.1300\n",
      "Epoch [15/3], Loss: 0.0607\n",
      "Epoch [16/3], Loss: 0.1378\n",
      "Epoch [17/3], Loss: 0.0253\n",
      "Epoch [18/3], Loss: 0.0945\n",
      "Epoch [19/3], Loss: 0.1846\n",
      "Epoch [20/3], Loss: 0.0713\n",
      "Epoch [21/3], Loss: 0.0637\n",
      "Epoch [22/3], Loss: 0.0822\n",
      "Epoch [23/3], Loss: 0.0889\n",
      "Epoch [24/3], Loss: 0.0854\n",
      "Epoch [25/3], Loss: 0.0508\n",
      "Epoch [26/3], Loss: 0.0245\n",
      "Epoch [27/3], Loss: 0.0597\n",
      "Epoch [28/3], Loss: 0.1098\n",
      "Epoch [29/3], Loss: 0.0475\n",
      "Epoch [30/3], Loss: 0.1687\n",
      "Epoch [31/3], Loss: 0.0411\n",
      "Epoch [32/3], Loss: 0.0670\n",
      "Epoch [33/3], Loss: 0.0514\n",
      "Epoch [34/3], Loss: 0.1364\n",
      "Epoch [35/3], Loss: 0.1127\n",
      "Epoch [36/3], Loss: 0.0813\n",
      "Epoch [37/3], Loss: 0.1229\n",
      "Epoch [38/3], Loss: 0.0797\n",
      "Epoch [39/3], Loss: 0.0664\n",
      "Epoch [40/3], Loss: 0.0995\n",
      "Epoch [41/3], Loss: 0.0686\n",
      "Epoch [42/3], Loss: 0.0892\n",
      "Epoch [43/3], Loss: 0.1554\n",
      "Epoch [44/3], Loss: 0.0639\n",
      "Epoch [45/3], Loss: 0.0527\n",
      "Epoch [46/3], Loss: 0.1388\n",
      "Epoch [47/3], Loss: 0.0633\n",
      "Epoch [48/3], Loss: 0.0627\n",
      "Epoch [49/3], Loss: 0.1249\n",
      "Epoch [50/3], Loss: 0.1091\n",
      "Epoch [51/3], Loss: 0.0168\n",
      "Epoch [52/3], Loss: 0.0686\n",
      "Epoch [53/3], Loss: 0.0846\n",
      "Epoch [54/3], Loss: 0.0347\n",
      "Epoch [55/3], Loss: 0.0962\n",
      "Epoch [56/3], Loss: 0.0694\n",
      "Epoch [57/3], Loss: 0.0366\n",
      "Epoch [58/3], Loss: 0.0618\n",
      "Epoch [59/3], Loss: 0.1205\n",
      "Epoch [60/3], Loss: 0.0789\n",
      "Epoch [61/3], Loss: 0.0395\n",
      "Epoch [62/3], Loss: 0.0344\n",
      "Epoch [63/3], Loss: 0.0789\n",
      "Epoch [64/3], Loss: 0.1039\n",
      "Epoch [65/3], Loss: 0.0279\n",
      "Epoch [66/3], Loss: 0.0694\n",
      "Epoch [67/3], Loss: 0.1309\n",
      "Epoch [68/3], Loss: 0.0760\n",
      "Epoch [69/3], Loss: 0.0909\n",
      "Epoch [70/3], Loss: 0.0449\n",
      "Epoch [71/3], Loss: 0.0449\n",
      "Epoch [72/3], Loss: 0.0293\n",
      "Epoch [73/3], Loss: 0.0330\n",
      "Epoch [74/3], Loss: 0.0421\n",
      "Epoch [75/3], Loss: 0.0605\n",
      "Epoch [76/3], Loss: 0.0727\n",
      "Epoch [77/3], Loss: 0.0843\n",
      "Epoch [78/3], Loss: 0.0514\n",
      "Epoch [79/3], Loss: 0.1012\n",
      "Epoch [80/3], Loss: 0.0754\n",
      "Epoch [81/3], Loss: 0.0317\n",
      "Epoch [82/3], Loss: 0.0802\n",
      "Epoch [83/3], Loss: 0.0408\n",
      "Epoch [84/3], Loss: 0.0959\n",
      "Epoch [85/3], Loss: 0.1310\n",
      "Epoch [86/3], Loss: 0.1346\n",
      "Epoch [87/3], Loss: 0.0922\n",
      "Epoch [88/3], Loss: 0.0923\n",
      "Epoch [89/3], Loss: 0.0467\n",
      "Epoch [90/3], Loss: 0.0648\n",
      "Epoch [91/3], Loss: 0.0845\n",
      "Epoch [92/3], Loss: 0.0354\n",
      "Epoch [93/3], Loss: 0.0439\n",
      "Epoch [94/3], Loss: 0.0591\n",
      "Epoch [95/3], Loss: 0.1118\n",
      "Epoch [96/3], Loss: 0.0544\n",
      "Epoch [97/3], Loss: 0.0561\n",
      "Epoch [98/3], Loss: 0.0371\n",
      "Epoch [99/3], Loss: 0.0564\n",
      "Epoch [100/3], Loss: 0.0357\n",
      "Epoch [101/3], Loss: 0.0636\n",
      "Epoch [102/3], Loss: 0.0851\n",
      "Epoch [103/3], Loss: 0.0237\n",
      "Epoch [104/3], Loss: 0.0165\n",
      "Epoch [105/3], Loss: 0.0974\n",
      "Epoch [106/3], Loss: 0.0884\n",
      "Epoch [107/3], Loss: 0.0909\n",
      "Epoch [108/3], Loss: 0.1218\n",
      "Epoch [109/3], Loss: 0.0490\n",
      "Epoch [110/3], Loss: 0.0482\n",
      "Epoch [111/3], Loss: 0.0470\n",
      "Epoch [112/3], Loss: 0.0606\n",
      "Epoch [113/3], Loss: 0.0159\n",
      "Epoch [114/3], Loss: 0.0572\n",
      "Epoch [115/3], Loss: 0.1089\n",
      "Epoch [116/3], Loss: 0.1058\n",
      "Epoch [117/3], Loss: 0.0482\n",
      "Epoch [118/3], Loss: 0.1140\n",
      "Epoch [119/3], Loss: 0.0877\n",
      "Epoch [120/3], Loss: 0.0905\n",
      "Epoch [121/3], Loss: 0.2247\n",
      "Epoch [122/3], Loss: 0.0748\n",
      "Epoch [123/3], Loss: 0.0534\n",
      "Epoch [124/3], Loss: 0.0329\n",
      "Epoch [125/3], Loss: 0.0705\n",
      "Epoch [126/3], Loss: 0.0671\n",
      "Epoch [127/3], Loss: 0.0276\n",
      "Epoch [128/3], Loss: 0.0591\n",
      "Epoch [129/3], Loss: 0.0634\n",
      "Epoch [130/3], Loss: 0.0525\n",
      "Epoch [131/3], Loss: 0.0243\n",
      "Epoch [132/3], Loss: 0.1092\n",
      "Epoch [133/3], Loss: 0.0705\n",
      "Epoch [134/3], Loss: 0.0639\n",
      "Epoch [135/3], Loss: 0.0888\n",
      "Epoch [136/3], Loss: 0.1293\n",
      "Epoch [137/3], Loss: 0.0182\n",
      "Epoch [138/3], Loss: 0.0676\n",
      "Epoch [139/3], Loss: 0.0307\n",
      "Epoch [140/3], Loss: 0.0196\n",
      "Epoch [141/3], Loss: 0.0563\n",
      "Epoch [142/3], Loss: 0.0250\n",
      "Epoch [143/3], Loss: 0.0289\n",
      "Epoch [144/3], Loss: 0.0707\n",
      "Epoch [145/3], Loss: 0.0918\n",
      "Epoch [146/3], Loss: 0.0204\n",
      "Epoch [147/3], Loss: 0.1048\n",
      "Epoch [148/3], Loss: 0.0351\n",
      "Epoch [149/3], Loss: 0.0461\n",
      "Epoch [150/3], Loss: 0.1032\n",
      "Epoch [151/3], Loss: 0.0671\n",
      "Epoch [152/3], Loss: 0.0397\n",
      "Epoch [153/3], Loss: 0.0448\n",
      "Epoch [154/3], Loss: 0.0338\n",
      "Epoch [155/3], Loss: 0.0518\n",
      "Epoch [156/3], Loss: 0.0318\n",
      "Epoch [157/3], Loss: 0.1152\n",
      "Epoch [158/3], Loss: 0.0293\n",
      "Epoch [159/3], Loss: 0.0638\n",
      "Epoch [160/3], Loss: 0.0454\n",
      "Epoch [161/3], Loss: 0.1101\n",
      "Epoch [162/3], Loss: 0.1269\n",
      "Epoch [163/3], Loss: 0.0115\n",
      "Epoch [164/3], Loss: 0.0242\n",
      "Epoch [165/3], Loss: 0.0591\n",
      "Epoch [166/3], Loss: 0.1172\n",
      "Epoch [167/3], Loss: 0.0508\n",
      "Epoch [168/3], Loss: 0.0583\n",
      "Epoch [169/3], Loss: 0.1318\n",
      "Epoch [170/3], Loss: 0.0824\n",
      "Epoch [171/3], Loss: 0.0228\n",
      "Epoch [172/3], Loss: 0.0143\n",
      "Epoch [173/3], Loss: 0.0392\n",
      "Epoch [174/3], Loss: 0.0565\n",
      "Epoch [175/3], Loss: 0.0755\n",
      "Epoch [176/3], Loss: 0.0540\n",
      "Epoch [177/3], Loss: 0.0342\n",
      "Epoch [178/3], Loss: 0.0511\n",
      "Epoch [179/3], Loss: 0.0841\n",
      "Epoch [180/3], Loss: 0.0280\n",
      "Epoch [181/3], Loss: 0.0178\n",
      "Epoch [182/3], Loss: 0.0908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_103000/2993780433.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtotal_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \"\"\"\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jack0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 뉴럴 네트워크 모델 학습\n",
    "total_epochs = 3\n",
    "for epoch in range(len(train_loader)):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        ce_loss = CELoss(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        sgd_optimizer.zero_grad()\n",
    "        ce_loss.backward() # Back propagation\n",
    "        sgd_optimizer.step() # optimizer 작동\n",
    "            \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, total_epochs, ce_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정규화를 위해 사용 -=> 서로 연결된 연결망(layer)에서 0부터 1 사이의 확률로 뉴런을 제거(drop)하는 기법\n",
    "  * 어떤 특정한 설명변수 Feature만을 과도하게 집중하여 학습함으로써 발생할 수 있는 과대적합(Overfitting)을 방지\n",
    "* 뉴럴 네트워크 후에, 활성화 함수 전에 많이 사용\n",
    "* p : 각 element들이 0이 될 확률 => 얼마나 끊어줄 것인가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout 추가\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.mlp1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.5) # p : probability of an element to be zeroed. Default: 0.5\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mlp2 = nn.Linear(hidden_size, num_classes)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mlp1(x)\n",
    "        out = self.dropout(out) # dropout 추가!\n",
    "        out = self.relu(out)\n",
    "        out = self.mlp2(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(input_size=28*28*1, hidden_size=100, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 배치 정규화는 평균과 분산을 조정하는 과정이 별도의 과정으로 떼어진 것이 아니라, 신경망 안에 포함되어 학습 시 평균과 분산을 조정하는 과정\n",
    "* 즉, 각 레이어마다 정규화 하는 레이어를 두어, 변형된 분포가 나오지 않도록 조절\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization 추가\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.mlp1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size) # batch normalization 1d*(1차원) : BatchNorm1d\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mlp2 = nn.Linear(hidden_size, num_classes)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mlp1(x)\n",
    "        out = self.dropout(out) # batch normalization 추가!\n",
    "        out = self.relu(out)\n",
    "        out = self.mlp2(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(input_size=28*28*1, hidden_size=100, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2e2bce360819f88e39cac25630043f28fab3036cf9216375971b4d97393a103"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
