{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # 사용할 Layers\n",
    "        self.mlp1 = layers.Dense(hidden_1, activation=tf.nn.relu) # 첫번째 Layer\n",
    "        self.mlp2 = layers.Dense(hidden_2, activation=\"relu\") # 둘 다 사용 가능\n",
    "        self.mlp3 = layers.Dense(num_classes) # actiavtion=\"softmax\"\n",
    "\n",
    "    # tensorflow에서는 forward가 아닌 call을 사용\n",
    "    def call(self, x):\n",
    "        print('x:',x.shape)\n",
    "        x = self.mlp1(x) # 첫 레이어를 거친 값\n",
    "        print('x2:',x.shape)\n",
    "        x = self.mlp2(x)\n",
    "        print('x3:',x.shape)\n",
    "        x = self.mlp3(x)\n",
    "        print('x4:',x.shape)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 임의의 데이터로 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (1, 784)\n",
      "----------------------------------------\n",
      "x: (1, 784)\n",
      "x2: (1, 128)\n",
      "x3: (1, 256)\n",
      "x4: (1, 10)\n",
      "pred: tf.Tensor(\n",
      "[[0.1029661  0.04044169 0.09568977 0.11849261 0.04040658 0.23408753\n",
      "  0.15327367 0.03012977 0.09202974 0.09248251]], shape=(1, 10), dtype=float32)\n",
      "----------------------------------------\n",
      "y hat: tf.Tensor([5], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "data = tf.random.normal(shape=(1,28*28), mean=0., stddev=1.)\n",
    "print('data:', data.shape)\n",
    "print('-'*40)\n",
    "\n",
    "pred = model(data) # softmax를 거쳐 0과 1 사이의 값\n",
    "print('pred:', pred)\n",
    "print('-'*40)\n",
    "\n",
    "y_hat = tf.argmax(pred, 1) # 최대값인 하나를 인덱스로\n",
    "print('y hat:', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(3,28,28), mean=0., stddev=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_fc: (3, 100)\n",
      "tf.Tensor(\n",
      "[ 0.5234126   0.62165135  2.0812209   0.0570631   0.09281635  0.04693154\n",
      " -1.533257   -1.5113839  -1.2004786  -0.47020486], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fc_layer = layers.Dense(100) # hidden layer의 size가 100\n",
    "x_after_fc = fc_layer(tf.reshape(x,(-1,28*28))) # Fully Connected Layer는 1D만 받음\n",
    "print('x_after_fc:',x_after_fc.shape)\n",
    "print(x_after_fc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_fc: (3, 100)\n",
      "tf.Tensor(\n",
      "[0.         0.         0.         0.11074781 0.9276475  0.2737847\n",
      " 0.9741174  0.         0.6939348  0.        ], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fc_layer = layers.Dense(100, activation='relu') # ReLU => 음수는 모두 0으로\n",
    "x_after_fc = fc_layer(tf.reshape(x,(-1,28*28)))\n",
    "print('x_after_fc:',x_after_fc.shape)\n",
    "print(x_after_fc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.52438647, -0.7313995 , -0.17398156,  1.3262413 ,  0.8385133 ,\n",
       "        -0.08098479, -0.17513578, -1.5020161 ,  0.2095927 , -0.99065006]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(1,10), mean=0., stddev=1.)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tf.Tensor(\n",
      "[[0.13324046 0.0379538  0.06627333 0.29708263 0.18241465 0.0727322\n",
      "  0.06619688 0.01756227 0.09725747 0.02928628]], shape=(1, 10), dtype=float32)\n",
      "sum of pred: tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred = tf.nn.softmax(x)\n",
    "print('pred:',pred)\n",
    "print('sum of pred:',tf.reduce_sum(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = layers.Dense(hidden_1, activation=tf.nn.relu)\n",
    "        self.fc2 = layers.Dense(hidden_2, activation=tf.nn.relu)\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameter : 사람이 정의해 주어야 하는 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "model = NeuralNet(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sequantial : 한번에 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet2, self).__init__() # 상속\n",
    "        sequential = tf.keras.Sequential([\n",
    "            layers.Dense(hidden_1, activation=tf.nn.relu),\n",
    "            layers.Dense(hidden_2, activation=tf.nn.relu),\n",
    "            layers.Dense(num_classes, activation=tf.nn.softmax),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "model = NeuralNet2(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential + add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sequential을 먼저 선언하고 Layer를 추가해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compile() : 모델을 기계가 이해할 수 있도록 컴파일 하는 것 => 손실 함수와 옵티마이저, 메트릭 함수를 선택\n",
    "  * optimizer = 훈련 과정을 설정하는 옵티마이저를 설정합니다.\n",
    "  * loss = 훈련 과정에서 사용할 손실 함수(loss function)를 설정합니다.\n",
    "  * metrics = 훈련을 모니터링하기 위한 지표를 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # labels가 one-hot일 경우 사용\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', # labels가 integer일 경우 사용\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), # learning rate 설정\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2e2bce360819f88e39cac25630043f28fab3036cf9216375971b4d97393a103"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
