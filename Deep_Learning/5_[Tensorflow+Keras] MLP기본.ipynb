{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.mlp1 = layers.Dense(hidden_1, activation=tf.nn.relu)\n",
    "        self.mlp2 = layers.Dense(hidden_2, activation=\"relu\") # 둘 다 사용 가능\n",
    "        self.mlp3 = layers.Dense(num_classes) # actiavtion=\"softmax\"\n",
    "\n",
    "    def call(self, x):\n",
    "        print('x:',x.shape)\n",
    "        x = self.mlp1(x)\n",
    "        print('x2:',x.shape)\n",
    "        x = self.mlp2(x)\n",
    "        print('x3:',x.shape)\n",
    "        x = self.mlp3(x)\n",
    "        print('x4:',x.shape)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 19:07:02.079342: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-19 19:07:02.079559: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (1, 784)\n",
      "----------------------------------------\n",
      "x: (1, 784)\n",
      "x2: (1, 128)\n",
      "x3: (1, 256)\n",
      "x4: (1, 10)\n",
      "pred: tf.Tensor(\n",
      "[[0.13949569 0.13945116 0.12087785 0.28384194 0.08963875 0.02253515\n",
      "  0.01823299 0.0438124  0.02067833 0.12143576]], shape=(1, 10), dtype=float32)\n",
      "----------------------------------------\n",
      "y hat: tf.Tensor([3], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "data = tf.random.normal(shape=(1,28*28), mean=0., stddev=1.)\n",
    "print('data:', data.shape)\n",
    "print('-'*40)\n",
    "\n",
    "pred = model(data)\n",
    "print('pred:', pred)\n",
    "print('-'*40)\n",
    "\n",
    "y_hat = tf.argmax(pred, 1)\n",
    "print('y hat:', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(3,28,28), mean=0., stddev=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_fc: (3, 100)\n",
      "tf.Tensor(\n",
      "[ 2.0233107  -2.6097698  -0.73857266  1.1991029   2.8984256   0.18412846\n",
      " -0.03651004 -1.3617487  -0.09439725  0.9603974 ], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fc_layer = layers.Dense(100)\n",
    "x_after_fc = fc_layer(tf.reshape(x,(-1,28*28)))\n",
    "print('x_after_fc:',x_after_fc.shape)\n",
    "print(x_after_fc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_fc: (3, 100)\n",
      "tf.Tensor(\n",
      "[0.03649342 0.5683936  1.1797311  0.56895393 0.         1.9251724\n",
      " 0.         0.         1.4246778  0.87268186], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fc_layer = layers.Dense(100, activation='relu')\n",
    "x_after_fc = fc_layer(tf.reshape(x,(-1,28*28)))\n",
    "print('x_after_fc:',x_after_fc.shape)\n",
    "print(x_after_fc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.10982104, -1.277446  ,  0.48827   , -0.8570649 , -1.3606479 ,\n",
       "         0.5870613 ,  2.6384974 , -0.3602169 ,  0.00689749, -0.78907144]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(1,10), mean=0., stddev=1.)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tf.Tensor(\n",
      "[[0.05153955 0.01287236 0.07524869 0.0195987  0.0118447  0.08306221\n",
      "  0.6461462  0.03221111 0.04649877 0.02097763]], shape=(1, 10), dtype=float32)\n",
      "sum of pred: tf.Tensor(0.9999999, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred = tf.nn.softmax(x)\n",
    "print('pred:',pred)\n",
    "print('sum of pred:',tf.reduce_sum(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = layers.Dense(hidden_1, activation=tf.nn.relu)\n",
    "        self.fc2 = layers.Dense(hidden_2, activation=tf.nn.relu)\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "model = NeuralNet(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        sequential = tf.keras.Sequential([\n",
    "            layers.Dense(hidden_1, activation=tf.nn.relu),\n",
    "            layers.Dense(hidden_2, activation=tf.nn.relu),\n",
    "            layers.Dense(num_classes, activation=tf.nn.softmax),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "model = NeuralNet2(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential + add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # labels가 one-hot일 경우 사용\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', # labels가 integer일 경우 사용\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), # learning rate 설정\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu",
   "language": "python",
   "name": "minkyu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
