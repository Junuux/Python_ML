{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers, optimizers, datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------train----------------\n",
      "images: (60000, 28, 28) 0 255\n",
      "labels: (60000,)\n",
      "----------------test----------------\n",
      "images: (10000, 28, 28) 0 255\n",
      "labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "(images_train,labels_train), (images_test,labels_test) = datasets.mnist.load_data()\n",
    "print('----------------train----------------')\n",
    "print('images:',images_train.shape, images_train.min(), images_train.max()) # 모든 이미지는 픽셀 값으로 이루어짐\n",
    "print('labels:',labels_train.shape)\n",
    "print('----------------test----------------')\n",
    "print('images:',images_test.shape, images_test.min(), images_test.max())\n",
    "print('labels:',labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0,255] to [0,1]\n",
    "\n",
    "images_train = images_train/255.\n",
    "images_test = images_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sequential로 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten : 이미지 크기를 펴준다\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax') # 최종 10개 레이블이고 확률값을 얻기 위해 softmax\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* label이 integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.3014 - accuracy: 0.1124\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 2.3014 - accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1adfab4ad30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(images_train, labels_train, epochs=3, batch_size=64) # 60000/64=937.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 평가\n",
    "  * verbose : 결과 출력 종류 (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 2.3011 - accuracy: 0.1135 - 562ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.301116943359375, 0.11349999904632568)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(images_test,  labels_test, verbose=2)\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.11349999904632568\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_dropout(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet_dropout, self).__init__()\n",
    "        self.flatten = layers.Flatten(input_shape=(28,28)) # flatten 해 주어야 모델 학습 가능\n",
    "        self.fc1 = layers.Dense(hidden_1) # 첫 fc layer\n",
    "        self.dropout1 = layers.Dropout(0.5) # Dropout(p)에서 p : 얼만큼 삭제할 것인가\n",
    "        self.fc2 = layers.Dense(hidden_2)\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x) # dropout : fc_layer와 활성화 함수 사이에\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "model_dropout = NeuralNet_dropout(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 12s 10ms/step - loss: 2.3016 - accuracy: 0.1101\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 2.3014 - accuracy: 0.1124\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 2.3014 - accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1adfcfa54f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dropout.fit(images_train, labels_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 2.3010 - accuracy: 0.1135 - 1s/epoch - 3ms/step\n",
      "test accuracy: 0.11349999904632568\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_dropout.evaluate(images_test,  labels_test, verbose=2)\n",
    "\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_bn(Model):\n",
    "    def __init__(self, hidden_1, hidden_2, num_classes):\n",
    "        super(NeuralNet_bn, self).__init__()\n",
    "\n",
    "        self.flatten = layers.Flatten(input_shape=(28,28))\n",
    "        self.fc1 = layers.Dense(hidden_1)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.fc2 = layers.Dense(hidden_2)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameter : 인간이 수정할 수 있는 값\n",
    "  * num_classes : mnist의 label이 10개라서 10이므로 이는 hyperparameter가 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 256\n",
    "num_classes = 10 # 데이터의 특성에 따라 정해짐\n",
    "\n",
    "model_bn = NeuralNet_bn(hidden_1, hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 9s 8ms/step - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 2.3014 - accuracy: 0.1124\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 2.3014 - accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1adfbc7a7c0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bn.fit(images_train, labels_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 3.6181 - accuracy: 0.1135 - 1s/epoch - 4ms/step\n",
      "test loss: 3.618062973022461\n",
      "test accuracy: 0.11349999904632568\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_bn.evaluate(images_test,  labels_test, verbose=2)\n",
    "\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2e2bce360819f88e39cac25630043f28fab3036cf9216375971b4d97393a103"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
