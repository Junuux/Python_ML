{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv2d : CONV Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원하는 이미지 사이즈에 따라 kernel size, stride, padding을 설정할 수 있음\n",
    "- 일반적으로 kernel=3, stride=1, padding=1을 하면 원래 이미지 사이즈가 유지되며 max pool등을 통해 이미지 사이즈를 반으로 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128, 1, 28, 28).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter : 1, out_channels : 32, stride=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_conv: torch.Size([128, 32, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv2d(1, 32, kernel_size=3) # filter : 1, out_channels : 32\n",
    "x_after_conv = conv_layer(x)\n",
    "print('x_after_conv:',x_after_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stride=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_conv: torch.Size([128, 32, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv2d(1, 32, kernel_size=3, stride=2)\n",
    "x_after_conv = conv_layer(x)\n",
    "print('x_after_conv:',x_after_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_conv: torch.Size([128, 32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "x_after_conv = conv_layer(x)\n",
    "print('x_after_conv:',x_after_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128, 1, 28, 28).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_maxpool: torch.Size([128, 1, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "x_after_maxpool = maxpool(x)\n",
    "print('x_after_maxpool:',x_after_maxpool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_maxpool: torch.Size([128, 1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "maxpool = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "x_after_maxpool = maxpool(x)\n",
    "print('x_after_maxpool:',x_after_maxpool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128, 1, 28, 28).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_avgpool: torch.Size([128, 1, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "x_after_avgpool = avgpool(x)\n",
    "print('x_after_avgpool:',x_after_avgpool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_avgpool: torch.Size([128, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "avgpool = nn.AvgPool2d(kernel_size=7, stride=7)\n",
    "x_after_avgpool = avgpool(x)\n",
    "print('x_after_avgpool:',x_after_avgpool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128, 1, 28, 28).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_after_global_avg_pool: torch.Size([128, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "global_avg_pool = nn.AdaptiveAvgPool2d((1,1)) # 이미지 출력 값을 (1,1)로 \n",
    "x_after_global_avg_pool = global_avg_pool(x)\n",
    "print('x_after_global_avg_pool:',x_after_global_avg_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MNIST 데이터셋 \n",
    "train_data = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) # input=1, output=32\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # input은 이전의 output channel이므로 32\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.mlp = nn.Linear(64*7*7, num_classes)  # print('out 2:',out.shape) # [128, 64, 7, 7]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        # print('out 1:',out.sha pe) # [128, 32, 14, 14]\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        # print('out 2:',out.shape) # [128, 64, 7, 7]\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.mlp(out) # 마지막 classification을 위해 필요\n",
    "        \n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN_Model(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELoss = nn.CrossEntropyLoss()\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 최적화 알고리즘 class 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iteration : 469\n",
      "Epoch [1/3], Loss: 0.0309\n",
      "Epoch [2/3], Loss: 0.0347\n",
      "Epoch [3/3], Loss: 0.0217\n"
     ]
    }
   ],
   "source": [
    "# 뉴럴 네트워크 모델 학습\n",
    "total_epochs = 3\n",
    "print('number of iteration :', len(train_loader))\n",
    "# epoch : 모든 데이터를 한 번 학습하는 단위\n",
    "for epoch in range(total_epochs):\n",
    "    # iteration : 한 'mini-batch' 단위의 데이터를 학습하는 단위\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # images : [mini-batch, 1, 28, 28]\n",
    "        # labels : [mini-batch]\n",
    "#         images = images.reshape(-1, 28*28).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        ce_loss = CELoss(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        adam_optimizer.zero_grad() # 다양한 optimization 기법 적용 가능\n",
    "        ce_loss.backward() # Back propagation\n",
    "        adam_optimizer.step() # optimizer 작동\n",
    "            \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, total_epochs, ce_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.48 %\n"
     ]
    }
   ],
   "source": [
    "# 학습이 끝난 후 모델 성능 테스트\n",
    "# test에서는 back propagation 작업을 하지 않으므로 gradient를 계산하지 않도록 함 - 메모리의 효율성을 위해\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # gradient 계산하지 않도록 하는 코드\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델을 model_CNN.ckpt라는 이름으로 저장\n",
    "torch.save(model.state_dict(), 'model_CNN.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습한 모델 다시 불러오기(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_CNN.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2e2bce360819f88e39cac25630043f28fab3036cf9216375971b4d97393a103"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
